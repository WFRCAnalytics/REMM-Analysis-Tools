{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create tables for:\n",
    "- [COUNTY_NAME] 4 Counties - 4 tables (can use loop) - GvR, CvN\n",
    "- [MPO] By MPO - WFRC only & WFRC+MAG  2 tables - GvR, CvN\n",
    "- [CITY_AREA] For each City Area - Many tables (loop required) - GvR, CvN\n",
    "- [CENTER_TYPE] By Center Type - city, urban, neighborhood, town, employment  5 tables - RVG\n",
    "- [INFILL1990] in/out of Infill 1990 boundary - Filter to WFRC only -  1 table - RVG, CvN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "# show all columns\n",
    "pd.options.display.max_columns = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = os.path.join(r'.\\Outputs', 'Growth_Summaries')\n",
    "if not os.path.exists(outputs):\n",
    "    os.makedirs(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parcel Equivalency Table\n",
    "eq = pd.read_csv(r\".\\Inputs\\parcel_eq_v3.csv\")\n",
    "centers_eq_ids = eq[eq['CENTER_NAME'].isna() == False]['parcel_id'].to_list()\n",
    "infill_eq_ids = eq[eq['INFILL1990']==1]['parcel_id'].to_list()\n",
    "\n",
    "# REMM project older\n",
    "remm_folder = r\"E:\\Projects\\REMM2_For_Python3_Internal_Use\"\n",
    "name = 'joshpc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm_folder = os.path.join(remm_folder, 'REMMRun\\\\Progression_Metrics')\n",
    "csvs = glob.glob(os.path.join(pm_folder, 'run_*_year_*_parcel_progression_metrics.csv'))\n",
    "csvs = [csv for csv in csvs if 'base' not in csv]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# By County"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['UTAH', 'WEBER', 'SALT LAKE', 'DAVIS']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "county_outputs = os.path.join(outputs, 'County')\n",
    "if not os.path.exists(county_outputs):\n",
    "    os.makedirs(county_outputs)\n",
    "\n",
    "counties = list(set(eq[eq['COUNTY_NAME'].isna()==False]['COUNTY_NAME'].to_list()))\n",
    "counties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "for county in counties:\n",
    "     \n",
    "     summary = pd.DataFrame()\n",
    "     for csv in csvs:\n",
    "          df = pd.read_csv(csv)\n",
    "          df = df.merge(eq, on='parcel_id', how='left')\n",
    "          df = df[df['COUNTY_NAME']== county].copy()\n",
    "\n",
    "          year = os.path.basename(csv).split('_')[3]\n",
    "\n",
    "          #--------------------------------------\n",
    "          # Centered(C) vs Noncentered (NC)\n",
    "          #--------------------------------------\n",
    "\n",
    "          # separate into centered and noncentered\n",
    "          C = df[df['parcel_id'].isin(centers_eq_ids)==True].copy()\n",
    "          NC = df[df['parcel_id'].isin(centers_eq_ids)==False].copy()\n",
    "\n",
    "          # new jobs (centered)\n",
    "          C_gov_edu_added = C['jobs_gov_edu_added'].sum()\n",
    "          C_jobs_health_added = C['jobs_health_added'].sum()\n",
    "          C_jobs_accom_food_added = C['jobs_accom_food_added'].sum()\n",
    "          C_jobs_manuf_added = C['jobs_manuf_added'].sum()\n",
    "          C_jobs_office_added = C['jobs_office_added'].sum()\n",
    "          C_jobs_other_added = C['jobs_other_added'].sum()\n",
    "          C_jobs_retail_added = C['jobs_retail_added'].sum()\n",
    "          C_jobs_wholesale_added = C['jobs_wholesale_added'].sum()\n",
    "\n",
    "          C_new_retail_jobs = C_jobs_accom_food_added + C_jobs_retail_added\n",
    "          C_new_industrial_jobs = C_jobs_manuf_added + C_jobs_wholesale_added\n",
    "          C_new_office_jobs = C_jobs_other_added + C_jobs_office_added + C_jobs_health_added + C_gov_edu_added\n",
    "          C_new_jobs = C_new_retail_jobs + C_new_industrial_jobs + C_new_office_jobs\n",
    "\n",
    "          # new jobs (non-centered)\n",
    "          NC_gov_edu_added = NC['jobs_gov_edu_added'].sum()\n",
    "          NC_jobs_health_added = NC['jobs_health_added'].sum()\n",
    "          NC_jobs_accom_food_added = NC['jobs_accom_food_added'].sum()\n",
    "          NC_jobs_manuf_added = NC['jobs_manuf_added'].sum()\n",
    "          NC_jobs_office_added = NC['jobs_office_added'].sum()\n",
    "          NC_jobs_other_added = NC['jobs_other_added'].sum()\n",
    "          NC_jobs_retail_added = NC['jobs_retail_added'].sum()\n",
    "          NC_jobs_wholesale_added = NC['jobs_wholesale_added'].sum()\n",
    "\n",
    "          NC_new_retail_jobs = NC_jobs_accom_food_added + NC_jobs_retail_added\n",
    "          NC_new_industrial_jobs = NC_jobs_manuf_added + NC_jobs_wholesale_added\n",
    "          NC_new_office_jobs = NC_jobs_other_added + NC_jobs_office_added + NC_jobs_health_added + NC_gov_edu_added\n",
    "          NC_new_jobs = NC_new_retail_jobs + NC_new_industrial_jobs + NC_new_office_jobs\n",
    "\n",
    "          # filter to new residential\n",
    "          C_new_sf = C[((C['was_developed']==1) | (C['was_redeveloped']==1)) & ((C['is_sf']==1))]\n",
    "          C_new_mf = C[((C['was_developed']==1) | (C['was_redeveloped']==1)) & ((C['is_mf']==1))]\n",
    "          NC_new_sf = NC[((NC['was_developed']==1) | (NC['was_redeveloped']==1)) & ((NC['is_sf']==1))]\n",
    "          NC_new_mf = NC[((NC['was_developed']==1) | (NC['was_redeveloped']==1)) & ((NC['is_mf']==1))]\n",
    "\n",
    "          # get the totals & counts\n",
    "          C_new_sf_units = C_new_sf['residential_units'].sum()\n",
    "          C_new_mf_units = C_new_mf['residential_units'].sum()\n",
    "          NC_new_sf_units = NC_new_sf['residential_units'].sum()\n",
    "          NC_new_mf_units = NC_new_mf['residential_units'].sum()\n",
    "          C_new_units = C_new_sf_units + C_new_mf_units\n",
    "          NC_new_units = NC_new_sf_units + NC_new_mf_units\n",
    "\n",
    "          #---------------------------\n",
    "          # Greenfield(G) vs Redev(R)\n",
    "          #---------------------------\n",
    "\n",
    "          # separate into Greenfield and Redev\n",
    "          G = df[df['was_developed']==1].copy()\n",
    "          R = df[df['was_redeveloped']==1].copy()\n",
    "\n",
    "          # new jobs (Greenfield)\n",
    "          G_gov_edu_added = G['jobs_gov_edu_added'].sum()\n",
    "          G_jobs_health_added = G['jobs_health_added'].sum()\n",
    "          G_jobs_accom_food_added = G['jobs_accom_food_added'].sum()\n",
    "          G_jobs_manuf_added = G['jobs_manuf_added'].sum()\n",
    "          G_jobs_office_added = G['jobs_office_added'].sum()\n",
    "          G_jobs_other_added = G['jobs_other_added'].sum()\n",
    "          G_jobs_retail_added = G['jobs_retail_added'].sum()\n",
    "          G_jobs_wholesale_added = G['jobs_wholesale_added'].sum()\n",
    "\n",
    "          G_new_retail_jobs = G_jobs_accom_food_added + G_jobs_retail_added\n",
    "          G_new_industrial_jobs = G_jobs_manuf_added + G_jobs_wholesale_added\n",
    "          G_new_office_jobs = G_jobs_other_added + G_jobs_office_added + G_jobs_health_added + G_gov_edu_added\n",
    "          G_new_jobs = G_new_retail_jobs + G_new_industrial_jobs + G_new_office_jobs\n",
    "\n",
    "          # new jobs (non-Redev)\n",
    "          R_gov_edu_added = R['jobs_gov_edu_added'].sum()\n",
    "          R_jobs_health_added = R['jobs_health_added'].sum()\n",
    "          R_jobs_accom_food_added = R['jobs_accom_food_added'].sum()\n",
    "          R_jobs_manuf_added = R['jobs_manuf_added'].sum()\n",
    "          R_jobs_office_added = R['jobs_office_added'].sum()\n",
    "          R_jobs_other_added = R['jobs_other_added'].sum()\n",
    "          R_jobs_retail_added = R['jobs_retail_added'].sum()\n",
    "          R_jobs_wholesale_added = R['jobs_wholesale_added'].sum()\n",
    "\n",
    "          R_new_retail_jobs = R_jobs_accom_food_added + R_jobs_retail_added\n",
    "          R_new_industrial_jobs = R_jobs_manuf_added + R_jobs_wholesale_added\n",
    "          R_new_office_jobs = R_jobs_other_added + R_jobs_office_added + R_jobs_health_added + R_gov_edu_added\n",
    "          R_new_jobs = R_new_retail_jobs + R_new_industrial_jobs + R_new_office_jobs\n",
    "\n",
    "          # filter to new residential\n",
    "          G_new_sf = G[((G['was_developed']==1) | (G['was_redeveloped']==1)) & ((G['is_sf']==1))]\n",
    "          G_new_mf = G[((G['was_developed']==1) | (G['was_redeveloped']==1)) & ((G['is_mf']==1))]\n",
    "          R_new_sf = R[((R['was_developed']==1) | (R['was_redeveloped']==1)) & ((R['is_sf']==1))]\n",
    "          R_new_mf = R[((R['was_developed']==1) | (R['was_redeveloped']==1)) & ((R['is_mf']==1))]\n",
    "\n",
    "          # get the totals & counts\n",
    "          G_new_sf_units = G_new_sf['residential_units'].sum()\n",
    "          G_new_mf_units = G_new_mf['residential_units'].sum()\n",
    "          R_new_sf_units = R_new_sf['residential_units'].sum()\n",
    "          R_new_mf_units = R_new_mf['residential_units'].sum()\n",
    "          G_new_units = G_new_sf_units + G_new_mf_units\n",
    "          R_new_units = R_new_sf_units + R_new_mf_units\n",
    "\n",
    "\n",
    "          # append current data to final dataframe \n",
    "          d = {\n",
    "               'year': [year],\n",
    "\n",
    "               'C_new_res_units': [C_new_units], 'NC_new_res_units': [NC_new_units], \n",
    "               'C_new_sf_units': [C_new_sf_units], 'NC_new_sf_units': [NC_new_sf_units],\n",
    "               'C_new_mf_units': [C_new_mf_units], 'NC_new_mf_units': [NC_new_mf_units],\n",
    "               'C_new_jobs':[C_new_jobs], 'NC_new_jobs':[NC_new_jobs],\n",
    "               'C_new_retail_jobs':[C_new_retail_jobs], 'NC_new_retail_jobs':[NC_new_retail_jobs], \n",
    "               'C_new_industrial_jobs':[C_new_industrial_jobs],'NC_new_industrial_jobs':[NC_new_industrial_jobs], \n",
    "               'C_new_office_jobs':[C_new_office_jobs], 'NC_new_office_jobs':[NC_new_office_jobs],\n",
    "\n",
    "               'G_new_res_units': [G_new_units], 'R_new_res_units': [R_new_units], \n",
    "               'G_new_sf_units': [G_new_sf_units], 'R_new_sf_units': [R_new_sf_units],\n",
    "               'G_new_mf_units': [G_new_mf_units], 'R_new_mf_units': [R_new_mf_units],\n",
    "               'G_new_jobs':[G_new_jobs], 'R_new_jobs':[R_new_jobs],\n",
    "               'G_new_retail_jobs':[G_new_retail_jobs], 'R_new_retail_jobs':[R_new_retail_jobs], \n",
    "               'G_new_industrial_jobs':[G_new_industrial_jobs],'R_new_industrial_jobs':[R_new_industrial_jobs], \n",
    "               'G_new_office_jobs':[G_new_office_jobs], 'R_new_office_jobs':[R_new_office_jobs]\n",
    "               }\n",
    "\n",
    "          row = pd.DataFrame(data=d)\n",
    "          summary = pd.concat([summary, row]).astype(int)\n",
    "\n",
    "          # export\n",
    "          summary = summary[summary['year'] != 2019]\n",
    "          summary.to_csv(os.path.join(county_outputs, f'{county}_County_Growth_{name}.csv'), index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# By MPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpo_outputs = os.path.join(outputs, 'MPO')\n",
    "if not os.path.exists(mpo_outputs):\n",
    "    os.makedirs(mpo_outputs)\n",
    "\n",
    "wfrc_eq = eq[eq['MPO'] == 'WFRC']['parcel_id'].to_list()\n",
    "wfrc_mag_eq = eq['parcel_id'].to_list()\n",
    "\n",
    "mpos = [['WFRC', wfrc_eq],['WFRC_MAG', wfrc_mag_eq]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "for mpo in mpos:\n",
    "\n",
    "     summary = pd.DataFrame()\n",
    "     for csv in csvs:\n",
    "          df = pd.read_csv(csv)\n",
    "          df = df.merge(eq, on='parcel_id', how='left')\n",
    "          df = df[df['parcel_id'].isin(mpo[1]) == True].copy()\n",
    "\n",
    "          year = os.path.basename(csv).split('_')[3]\n",
    "\n",
    "          #--------------------------------------\n",
    "          # Centered(C) vs Noncentered (NC)\n",
    "          #--------------------------------------\n",
    "\n",
    "          # separate into centered and noncentered\n",
    "          C = df[df['parcel_id'].isin(centers_eq_ids)==True].copy()\n",
    "          NC = df[df['parcel_id'].isin(centers_eq_ids)==False].copy()\n",
    "\n",
    "          # new jobs (centered)\n",
    "          C_gov_edu_added = C['jobs_gov_edu_added'].sum()\n",
    "          C_jobs_health_added = C['jobs_health_added'].sum()\n",
    "          C_jobs_accom_food_added = C['jobs_accom_food_added'].sum()\n",
    "          C_jobs_manuf_added = C['jobs_manuf_added'].sum()\n",
    "          C_jobs_office_added = C['jobs_office_added'].sum()\n",
    "          C_jobs_other_added = C['jobs_other_added'].sum()\n",
    "          C_jobs_retail_added = C['jobs_retail_added'].sum()\n",
    "          C_jobs_wholesale_added = C['jobs_wholesale_added'].sum()\n",
    "\n",
    "          C_new_retail_jobs = C_jobs_accom_food_added + C_jobs_retail_added\n",
    "          C_new_industrial_jobs = C_jobs_manuf_added + C_jobs_wholesale_added\n",
    "          C_new_office_jobs = C_jobs_other_added + C_jobs_office_added + C_jobs_health_added + C_gov_edu_added\n",
    "          C_new_jobs = C_new_retail_jobs + C_new_industrial_jobs + C_new_office_jobs\n",
    "\n",
    "          # new jobs (non-centered)\n",
    "          NC_gov_edu_added = NC['jobs_gov_edu_added'].sum()\n",
    "          NC_jobs_health_added = NC['jobs_health_added'].sum()\n",
    "          NC_jobs_accom_food_added = NC['jobs_accom_food_added'].sum()\n",
    "          NC_jobs_manuf_added = NC['jobs_manuf_added'].sum()\n",
    "          NC_jobs_office_added = NC['jobs_office_added'].sum()\n",
    "          NC_jobs_other_added = NC['jobs_other_added'].sum()\n",
    "          NC_jobs_retail_added = NC['jobs_retail_added'].sum()\n",
    "          NC_jobs_wholesale_added = NC['jobs_wholesale_added'].sum()\n",
    "\n",
    "          NC_new_retail_jobs = NC_jobs_accom_food_added + NC_jobs_retail_added\n",
    "          NC_new_industrial_jobs = NC_jobs_manuf_added + NC_jobs_wholesale_added\n",
    "          NC_new_office_jobs = NC_jobs_other_added + NC_jobs_office_added + NC_jobs_health_added + NC_gov_edu_added\n",
    "          NC_new_jobs = NC_new_retail_jobs + NC_new_industrial_jobs + NC_new_office_jobs\n",
    "\n",
    "          # filter to new residential\n",
    "          C_new_sf = C[((C['was_developed']==1) | (C['was_redeveloped']==1)) & ((C['is_sf']==1))]\n",
    "          C_new_mf = C[((C['was_developed']==1) | (C['was_redeveloped']==1)) & ((C['is_mf']==1))]\n",
    "          NC_new_sf = NC[((NC['was_developed']==1) | (NC['was_redeveloped']==1)) & ((NC['is_sf']==1))]\n",
    "          NC_new_mf = NC[((NC['was_developed']==1) | (NC['was_redeveloped']==1)) & ((NC['is_mf']==1))]\n",
    "\n",
    "          # get the totals & counts\n",
    "          C_new_sf_units = C_new_sf['residential_units'].sum()\n",
    "          C_new_mf_units = C_new_mf['residential_units'].sum()\n",
    "          NC_new_sf_units = NC_new_sf['residential_units'].sum()\n",
    "          NC_new_mf_units = NC_new_mf['residential_units'].sum()\n",
    "          C_new_units = C_new_sf_units + C_new_mf_units\n",
    "          NC_new_units = NC_new_sf_units + NC_new_mf_units\n",
    "\n",
    "          #---------------------------\n",
    "          # Greenfield(G) vs Redev(R)\n",
    "          #---------------------------\n",
    "\n",
    "          # separate into Greenfield and Redev\n",
    "          G = df[df['was_developed']==1].copy()\n",
    "          R = df[df['was_redeveloped']==1].copy()\n",
    "\n",
    "          # new jobs (Greenfield)\n",
    "          G_gov_edu_added = G['jobs_gov_edu_added'].sum()\n",
    "          G_jobs_health_added = G['jobs_health_added'].sum()\n",
    "          G_jobs_accom_food_added = G['jobs_accom_food_added'].sum()\n",
    "          G_jobs_manuf_added = G['jobs_manuf_added'].sum()\n",
    "          G_jobs_office_added = G['jobs_office_added'].sum()\n",
    "          G_jobs_other_added = G['jobs_other_added'].sum()\n",
    "          G_jobs_retail_added = G['jobs_retail_added'].sum()\n",
    "          G_jobs_wholesale_added = G['jobs_wholesale_added'].sum()\n",
    "\n",
    "          G_new_retail_jobs = G_jobs_accom_food_added + G_jobs_retail_added\n",
    "          G_new_industrial_jobs = G_jobs_manuf_added + G_jobs_wholesale_added\n",
    "          G_new_office_jobs = G_jobs_other_added + G_jobs_office_added + G_jobs_health_added + G_gov_edu_added\n",
    "          G_new_jobs = G_new_retail_jobs + G_new_industrial_jobs + G_new_office_jobs\n",
    "\n",
    "          # new jobs (non-Redev)\n",
    "          R_gov_edu_added = R['jobs_gov_edu_added'].sum()\n",
    "          R_jobs_health_added = R['jobs_health_added'].sum()\n",
    "          R_jobs_accom_food_added = R['jobs_accom_food_added'].sum()\n",
    "          R_jobs_manuf_added = R['jobs_manuf_added'].sum()\n",
    "          R_jobs_office_added = R['jobs_office_added'].sum()\n",
    "          R_jobs_other_added = R['jobs_other_added'].sum()\n",
    "          R_jobs_retail_added = R['jobs_retail_added'].sum()\n",
    "          R_jobs_wholesale_added = R['jobs_wholesale_added'].sum()\n",
    "\n",
    "          R_new_retail_jobs = R_jobs_accom_food_added + R_jobs_retail_added\n",
    "          R_new_industrial_jobs = R_jobs_manuf_added + R_jobs_wholesale_added\n",
    "          R_new_office_jobs = R_jobs_other_added + R_jobs_office_added + R_jobs_health_added + R_gov_edu_added\n",
    "          R_new_jobs = R_new_retail_jobs + R_new_industrial_jobs + R_new_office_jobs\n",
    "\n",
    "          # filter to new residential\n",
    "          G_new_sf = G[((G['was_developed']==1) | (G['was_redeveloped']==1)) & ((G['is_sf']==1))]\n",
    "          G_new_mf = G[((G['was_developed']==1) | (G['was_redeveloped']==1)) & ((G['is_mf']==1))]\n",
    "          R_new_sf = R[((R['was_developed']==1) | (R['was_redeveloped']==1)) & ((R['is_sf']==1))]\n",
    "          R_new_mf = R[((R['was_developed']==1) | (R['was_redeveloped']==1)) & ((R['is_mf']==1))]\n",
    "\n",
    "          # get the totals & counts\n",
    "          G_new_sf_units = G_new_sf['residential_units'].sum()\n",
    "          G_new_mf_units = G_new_mf['residential_units'].sum()\n",
    "          R_new_sf_units = R_new_sf['residential_units'].sum()\n",
    "          R_new_mf_units = R_new_mf['residential_units'].sum()\n",
    "          G_new_units = G_new_sf_units + G_new_mf_units\n",
    "          R_new_units = R_new_sf_units + R_new_mf_units\n",
    "\n",
    "\n",
    "          # append current data to final dataframe \n",
    "          d = {\n",
    "               'year': [year],\n",
    "\n",
    "               'C_new_res_units': [C_new_units], 'NC_new_res_units': [NC_new_units], \n",
    "               'C_new_sf_units': [C_new_sf_units], 'NC_new_sf_units': [NC_new_sf_units],\n",
    "               'C_new_mf_units': [C_new_mf_units], 'NC_new_mf_units': [NC_new_mf_units],\n",
    "               'C_new_jobs':[C_new_jobs], 'NC_new_jobs':[NC_new_jobs],\n",
    "               'C_new_retail_jobs':[C_new_retail_jobs], 'NC_new_retail_jobs':[NC_new_retail_jobs], \n",
    "               'C_new_industrial_jobs':[C_new_industrial_jobs],'NC_new_industrial_jobs':[NC_new_industrial_jobs], \n",
    "               'C_new_office_jobs':[C_new_office_jobs], 'NC_new_office_jobs':[NC_new_office_jobs],\n",
    "\n",
    "               'G_new_res_units': [G_new_units], 'R_new_res_units': [R_new_units], \n",
    "               'G_new_sf_units': [G_new_sf_units], 'R_new_sf_units': [R_new_sf_units],\n",
    "               'G_new_mf_units': [G_new_mf_units], 'R_new_mf_units': [R_new_mf_units],\n",
    "               'G_new_jobs':[G_new_jobs], 'R_new_jobs':[R_new_jobs],\n",
    "               'G_new_retail_jobs':[G_new_retail_jobs], 'R_new_retail_jobs':[R_new_retail_jobs], \n",
    "               'G_new_industrial_jobs':[G_new_industrial_jobs],'R_new_industrial_jobs':[R_new_industrial_jobs], \n",
    "               'G_new_office_jobs':[G_new_office_jobs], 'R_new_office_jobs':[R_new_office_jobs]\n",
    "               }\n",
    "\n",
    "          row = pd.DataFrame(data=d)\n",
    "          summary = pd.concat([summary, row]).astype(int)\n",
    "\n",
    "          # export\n",
    "          summary = summary[summary['year'] != 2019]\n",
    "          summary.to_csv(os.path.join(mpo_outputs, f'{mpo[0]}_Growth_{name}.csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# By City Area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_area_outputs = os.path.join(outputs, 'City_Area')\n",
    "if not os.path.exists(city_area_outputs):\n",
    "    os.makedirs(city_area_outputs)\n",
    "\n",
    "cities = list(set(eq[eq['CITY_AREA'].isna()==False]['CITY_AREA'].to_list()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "95"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-32-b21b17677e26>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m      \u001b[0msummary\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m      \u001b[1;32mfor\u001b[0m \u001b[0mcsv\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcsvs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m           \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcsv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m           \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0meq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mon\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'parcel_id'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhow\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'left'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m           \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'CITY_AREA'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m==\u001b[0m \u001b[0mcity\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\jreynolds\\AppData\\Local\\ESRI\\conda\\envs\\arcgispro-py3-clone\\lib\\site-packages\\pandas\\util\\_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m                 )\n\u001b[1;32m--> 311\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    312\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\jreynolds\\AppData\\Local\\ESRI\\conda\\envs\\arcgispro-py3-clone\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    585\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 586\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    587\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    588\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\jreynolds\\AppData\\Local\\ESRI\\conda\\envs\\arcgispro-py3-clone\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    486\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    487\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mparser\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 488\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mparser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    489\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    490\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\jreynolds\\AppData\\Local\\ESRI\\conda\\envs\\arcgispro-py3-clone\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1045\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1046\u001b[0m         \u001b[0mnrows\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalidate_integer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"nrows\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1047\u001b[1;33m         \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcol_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1048\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1049\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mindex\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\jreynolds\\AppData\\Local\\ESRI\\conda\\envs\\arcgispro-py3-clone\\lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m    222\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    223\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlow_memory\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 224\u001b[1;33m                 \u001b[0mchunks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_low_memory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    225\u001b[0m                 \u001b[1;31m# destructive to chunks\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    226\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_concatenate_chunks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\jreynolds\\AppData\\Local\\ESRI\\conda\\envs\\arcgispro-py3-clone\\lib\\site-packages\\pandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\jreynolds\\AppData\\Local\\ESRI\\conda\\envs\\arcgispro-py3-clone\\lib\\site-packages\\pandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\jreynolds\\AppData\\Local\\ESRI\\conda\\envs\\arcgispro-py3-clone\\lib\\site-packages\\pandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_column_data\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\jreynolds\\AppData\\Local\\ESRI\\conda\\envs\\arcgispro-py3-clone\\lib\\site-packages\\pandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_tokens\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\jreynolds\\AppData\\Local\\ESRI\\conda\\envs\\arcgispro-py3-clone\\lib\\site-packages\\pandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_with_dtype\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\jreynolds\\AppData\\Local\\ESRI\\conda\\envs\\arcgispro-py3-clone\\lib\\site-packages\\pandas\\core\\dtypes\\common.py\u001b[0m in \u001b[0;36mis_extension_array_dtype\u001b[1;34m(arr_or_dtype)\u001b[0m\n\u001b[0;32m   1418\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1419\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1420\u001b[1;33m \u001b[1;32mdef\u001b[0m \u001b[0mis_extension_array_dtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marr_or_dtype\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mbool\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1421\u001b[0m     \"\"\"\n\u001b[0;32m   1422\u001b[0m     \u001b[0mCheck\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0man\u001b[0m \u001b[0mobject\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0ma\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0mextension\u001b[0m \u001b[0marray\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for city in cities:\n",
    "     \n",
    "     summary = pd.DataFrame()\n",
    "     for csv in csvs:\n",
    "          df = pd.read_csv(csv)\n",
    "          df = df.merge(eq, on='parcel_id', how='left')\n",
    "          df = df[df['CITY_AREA']== city].copy()\n",
    "\n",
    "          year = os.path.basename(csv).split('_')[3]\n",
    "\n",
    "          #--------------------------------------\n",
    "          # Centered(C) vs Noncentered (NC)\n",
    "          #--------------------------------------\n",
    "\n",
    "          # separate into centered and noncentered\n",
    "          C = df[df['parcel_id'].isin(centers_eq_ids)==True].copy()\n",
    "          NC = df[df['parcel_id'].isin(centers_eq_ids)==False].copy()\n",
    "\n",
    "          # new jobs (centered)\n",
    "          C_gov_edu_added = C['jobs_gov_edu_added'].sum()\n",
    "          C_jobs_health_added = C['jobs_health_added'].sum()\n",
    "          C_jobs_accom_food_added = C['jobs_accom_food_added'].sum()\n",
    "          C_jobs_manuf_added = C['jobs_manuf_added'].sum()\n",
    "          C_jobs_office_added = C['jobs_office_added'].sum()\n",
    "          C_jobs_other_added = C['jobs_other_added'].sum()\n",
    "          C_jobs_retail_added = C['jobs_retail_added'].sum()\n",
    "          C_jobs_wholesale_added = C['jobs_wholesale_added'].sum()\n",
    "\n",
    "          C_new_retail_jobs = C_jobs_accom_food_added + C_jobs_retail_added\n",
    "          C_new_industrial_jobs = C_jobs_manuf_added + C_jobs_wholesale_added\n",
    "          C_new_office_jobs = C_jobs_other_added + C_jobs_office_added + C_jobs_health_added + C_gov_edu_added\n",
    "          C_new_jobs = C_new_retail_jobs + C_new_industrial_jobs + C_new_office_jobs\n",
    "\n",
    "          # new jobs (non-centered)\n",
    "          NC_gov_edu_added = NC['jobs_gov_edu_added'].sum()\n",
    "          NC_jobs_health_added = NC['jobs_health_added'].sum()\n",
    "          NC_jobs_accom_food_added = NC['jobs_accom_food_added'].sum()\n",
    "          NC_jobs_manuf_added = NC['jobs_manuf_added'].sum()\n",
    "          NC_jobs_office_added = NC['jobs_office_added'].sum()\n",
    "          NC_jobs_other_added = NC['jobs_other_added'].sum()\n",
    "          NC_jobs_retail_added = NC['jobs_retail_added'].sum()\n",
    "          NC_jobs_wholesale_added = NC['jobs_wholesale_added'].sum()\n",
    "\n",
    "          NC_new_retail_jobs = NC_jobs_accom_food_added + NC_jobs_retail_added\n",
    "          NC_new_industrial_jobs = NC_jobs_manuf_added + NC_jobs_wholesale_added\n",
    "          NC_new_office_jobs = NC_jobs_other_added + NC_jobs_office_added + NC_jobs_health_added + NC_gov_edu_added\n",
    "          NC_new_jobs = NC_new_retail_jobs + NC_new_industrial_jobs + NC_new_office_jobs\n",
    "\n",
    "          # filter to new residential\n",
    "          C_new_sf = C[((C['was_developed']==1) | (C['was_redeveloped']==1)) & ((C['is_sf']==1))]\n",
    "          C_new_mf = C[((C['was_developed']==1) | (C['was_redeveloped']==1)) & ((C['is_mf']==1))]\n",
    "          NC_new_sf = NC[((NC['was_developed']==1) | (NC['was_redeveloped']==1)) & ((NC['is_sf']==1))]\n",
    "          NC_new_mf = NC[((NC['was_developed']==1) | (NC['was_redeveloped']==1)) & ((NC['is_mf']==1))]\n",
    "\n",
    "          # get the totals & counts\n",
    "          C_new_sf_units = C_new_sf['residential_units'].sum()\n",
    "          C_new_mf_units = C_new_mf['residential_units'].sum()\n",
    "          NC_new_sf_units = NC_new_sf['residential_units'].sum()\n",
    "          NC_new_mf_units = NC_new_mf['residential_units'].sum()\n",
    "          C_new_units = C_new_sf_units + C_new_mf_units\n",
    "          NC_new_units = NC_new_sf_units + NC_new_mf_units\n",
    "\n",
    "          #---------------------------\n",
    "          # Greenfield(G) vs Redev(R)\n",
    "          #---------------------------\n",
    "\n",
    "          # separate into Greenfield and Redev\n",
    "          G = df[df['was_developed']==1].copy()\n",
    "          R = df[df['was_redeveloped']==1].copy()\n",
    "\n",
    "          # new jobs (Greenfield)\n",
    "          G_gov_edu_added = G['jobs_gov_edu_added'].sum()\n",
    "          G_jobs_health_added = G['jobs_health_added'].sum()\n",
    "          G_jobs_accom_food_added = G['jobs_accom_food_added'].sum()\n",
    "          G_jobs_manuf_added = G['jobs_manuf_added'].sum()\n",
    "          G_jobs_office_added = G['jobs_office_added'].sum()\n",
    "          G_jobs_other_added = G['jobs_other_added'].sum()\n",
    "          G_jobs_retail_added = G['jobs_retail_added'].sum()\n",
    "          G_jobs_wholesale_added = G['jobs_wholesale_added'].sum()\n",
    "\n",
    "          G_new_retail_jobs = G_jobs_accom_food_added + G_jobs_retail_added\n",
    "          G_new_industrial_jobs = G_jobs_manuf_added + G_jobs_wholesale_added\n",
    "          G_new_office_jobs = G_jobs_other_added + G_jobs_office_added + G_jobs_health_added + G_gov_edu_added\n",
    "          G_new_jobs = G_new_retail_jobs + G_new_industrial_jobs + G_new_office_jobs\n",
    "\n",
    "          # new jobs (non-Redev)\n",
    "          R_gov_edu_added = R['jobs_gov_edu_added'].sum()\n",
    "          R_jobs_health_added = R['jobs_health_added'].sum()\n",
    "          R_jobs_accom_food_added = R['jobs_accom_food_added'].sum()\n",
    "          R_jobs_manuf_added = R['jobs_manuf_added'].sum()\n",
    "          R_jobs_office_added = R['jobs_office_added'].sum()\n",
    "          R_jobs_other_added = R['jobs_other_added'].sum()\n",
    "          R_jobs_retail_added = R['jobs_retail_added'].sum()\n",
    "          R_jobs_wholesale_added = R['jobs_wholesale_added'].sum()\n",
    "\n",
    "          R_new_retail_jobs = R_jobs_accom_food_added + R_jobs_retail_added\n",
    "          R_new_industrial_jobs = R_jobs_manuf_added + R_jobs_wholesale_added\n",
    "          R_new_office_jobs = R_jobs_other_added + R_jobs_office_added + R_jobs_health_added + R_gov_edu_added\n",
    "          R_new_jobs = R_new_retail_jobs + R_new_industrial_jobs + R_new_office_jobs\n",
    "\n",
    "          # filter to new residential\n",
    "          G_new_sf = G[((G['was_developed']==1) | (G['was_redeveloped']==1)) & ((G['is_sf']==1))]\n",
    "          G_new_mf = G[((G['was_developed']==1) | (G['was_redeveloped']==1)) & ((G['is_mf']==1))]\n",
    "          R_new_sf = R[((R['was_developed']==1) | (R['was_redeveloped']==1)) & ((R['is_sf']==1))]\n",
    "          R_new_mf = R[((R['was_developed']==1) | (R['was_redeveloped']==1)) & ((R['is_mf']==1))]\n",
    "\n",
    "          # get the totals & counts\n",
    "          G_new_sf_units = G_new_sf['residential_units'].sum()\n",
    "          G_new_mf_units = G_new_mf['residential_units'].sum()\n",
    "          R_new_sf_units = R_new_sf['residential_units'].sum()\n",
    "          R_new_mf_units = R_new_mf['residential_units'].sum()\n",
    "          G_new_units = G_new_sf_units + G_new_mf_units\n",
    "          R_new_units = R_new_sf_units + R_new_mf_units\n",
    "\n",
    "\n",
    "          # append current data to final dataframe \n",
    "          d = {\n",
    "               'year': [year],\n",
    "\n",
    "               'C_new_res_units': [C_new_units], 'NC_new_res_units': [NC_new_units], \n",
    "               'C_new_sf_units': [C_new_sf_units], 'NC_new_sf_units': [NC_new_sf_units],\n",
    "               'C_new_mf_units': [C_new_mf_units], 'NC_new_mf_units': [NC_new_mf_units],\n",
    "               'C_new_jobs':[C_new_jobs], 'NC_new_jobs':[NC_new_jobs],\n",
    "               'C_new_retail_jobs':[C_new_retail_jobs], 'NC_new_retail_jobs':[NC_new_retail_jobs], \n",
    "               'C_new_industrial_jobs':[C_new_industrial_jobs],'NC_new_industrial_jobs':[NC_new_industrial_jobs], \n",
    "               'C_new_office_jobs':[C_new_office_jobs], 'NC_new_office_jobs':[NC_new_office_jobs],\n",
    "\n",
    "               'G_new_res_units': [G_new_units], 'R_new_res_units': [R_new_units], \n",
    "               'G_new_sf_units': [G_new_sf_units], 'R_new_sf_units': [R_new_sf_units],\n",
    "               'G_new_mf_units': [G_new_mf_units], 'R_new_mf_units': [R_new_mf_units],\n",
    "               'G_new_jobs':[G_new_jobs], 'R_new_jobs':[R_new_jobs],\n",
    "               'G_new_retail_jobs':[G_new_retail_jobs], 'R_new_retail_jobs':[R_new_retail_jobs], \n",
    "               'G_new_industrial_jobs':[G_new_industrial_jobs],'R_new_industrial_jobs':[R_new_industrial_jobs], \n",
    "               'G_new_office_jobs':[G_new_office_jobs], 'R_new_office_jobs':[R_new_office_jobs]\n",
    "               }\n",
    "\n",
    "          row = pd.DataFrame(data=d)\n",
    "          summary = pd.concat([summary, row]).astype(int)\n",
    "\n",
    "          # export\n",
    "          summary = summary[summary['year'] != 2019]\n",
    "          city_no_space = city.replace(' ', '_')\n",
    "          summary.to_csv(os.path.join(city_area_outputs, f'{city_no_space}_{name}.csv'), index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# By Center Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Urban Center',\n",
       " 'City Center',\n",
       " 'Neighborhood Center',\n",
       " 'Educational Center',\n",
       " 'Industrial District',\n",
       " 'Special District',\n",
       " 'Employment District',\n",
       " 'Metropolitan Center']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "center_type_outputs = os.path.join(outputs, 'County')\n",
    "if not os.path.exists(center_type_outputs):\n",
    "    os.makedirs(center_type_outputs)\n",
    "\n",
    "center_types = list(set(eq[(eq['CENTER_TYPE'].isna()==False)]['CENTER_TYPE'].to_list()))\n",
    "center_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "for center_type in center_types:\n",
    "     \n",
    "     summary = pd.DataFrame()\n",
    "     for csv in csvs:\n",
    "          df = pd.read_csv(csv)\n",
    "          df = df.merge(eq, on='parcel_id', how='left')\n",
    "          df = df[df['CENTER_TYPE']== center_type].copy()\n",
    "\n",
    "          year = os.path.basename(csv).split('_')[3]\n",
    "\n",
    "          #--------------------------------------\n",
    "          # Centered(C) vs Noncentered (NC)\n",
    "          #--------------------------------------\n",
    "\n",
    "          # separate into centered and noncentered\n",
    "          C = df[df['parcel_id'].isin(centers_eq_ids)==True].copy()\n",
    "          NC = df[df['parcel_id'].isin(centers_eq_ids)==False].copy()\n",
    "\n",
    "          # new jobs (centered)\n",
    "          C_gov_edu_added = C['jobs_gov_edu_added'].sum()\n",
    "          C_jobs_health_added = C['jobs_health_added'].sum()\n",
    "          C_jobs_accom_food_added = C['jobs_accom_food_added'].sum()\n",
    "          C_jobs_manuf_added = C['jobs_manuf_added'].sum()\n",
    "          C_jobs_office_added = C['jobs_office_added'].sum()\n",
    "          C_jobs_other_added = C['jobs_other_added'].sum()\n",
    "          C_jobs_retail_added = C['jobs_retail_added'].sum()\n",
    "          C_jobs_wholesale_added = C['jobs_wholesale_added'].sum()\n",
    "\n",
    "          C_new_retail_jobs = C_jobs_accom_food_added + C_jobs_retail_added\n",
    "          C_new_industrial_jobs = C_jobs_manuf_added + C_jobs_wholesale_added\n",
    "          C_new_office_jobs = C_jobs_other_added + C_jobs_office_added + C_jobs_health_added + C_gov_edu_added\n",
    "          C_new_jobs = C_new_retail_jobs + C_new_industrial_jobs + C_new_office_jobs\n",
    "\n",
    "          # new jobs (non-centered)\n",
    "          NC_gov_edu_added = NC['jobs_gov_edu_added'].sum()\n",
    "          NC_jobs_health_added = NC['jobs_health_added'].sum()\n",
    "          NC_jobs_accom_food_added = NC['jobs_accom_food_added'].sum()\n",
    "          NC_jobs_manuf_added = NC['jobs_manuf_added'].sum()\n",
    "          NC_jobs_office_added = NC['jobs_office_added'].sum()\n",
    "          NC_jobs_other_added = NC['jobs_other_added'].sum()\n",
    "          NC_jobs_retail_added = NC['jobs_retail_added'].sum()\n",
    "          NC_jobs_wholesale_added = NC['jobs_wholesale_added'].sum()\n",
    "\n",
    "          NC_new_retail_jobs = NC_jobs_accom_food_added + NC_jobs_retail_added\n",
    "          NC_new_industrial_jobs = NC_jobs_manuf_added + NC_jobs_wholesale_added\n",
    "          NC_new_office_jobs = NC_jobs_other_added + NC_jobs_office_added + NC_jobs_health_added + NC_gov_edu_added\n",
    "          NC_new_jobs = NC_new_retail_jobs + NC_new_industrial_jobs + NC_new_office_jobs\n",
    "\n",
    "          # filter to new residential\n",
    "          C_new_sf = C[((C['was_developed']==1) | (C['was_redeveloped']==1)) & ((C['is_sf']==1))]\n",
    "          C_new_mf = C[((C['was_developed']==1) | (C['was_redeveloped']==1)) & ((C['is_mf']==1))]\n",
    "          NC_new_sf = NC[((NC['was_developed']==1) | (NC['was_redeveloped']==1)) & ((NC['is_sf']==1))]\n",
    "          NC_new_mf = NC[((NC['was_developed']==1) | (NC['was_redeveloped']==1)) & ((NC['is_mf']==1))]\n",
    "\n",
    "          # get the totals & counts\n",
    "          C_new_sf_units = C_new_sf['residential_units'].sum()\n",
    "          C_new_mf_units = C_new_mf['residential_units'].sum()\n",
    "          NC_new_sf_units = NC_new_sf['residential_units'].sum()\n",
    "          NC_new_mf_units = NC_new_mf['residential_units'].sum()\n",
    "          C_new_units = C_new_sf_units + C_new_mf_units\n",
    "          NC_new_units = NC_new_sf_units + NC_new_mf_units\n",
    "\n",
    "          #---------------------------\n",
    "          # Greenfield(G) vs Redev(R)\n",
    "          #---------------------------\n",
    "\n",
    "          # separate into Greenfield and Redev\n",
    "          G = df[df['was_developed']==1].copy()\n",
    "          R = df[df['was_redeveloped']==1].copy()\n",
    "\n",
    "          # new jobs (Greenfield)\n",
    "          G_gov_edu_added = G['jobs_gov_edu_added'].sum()\n",
    "          G_jobs_health_added = G['jobs_health_added'].sum()\n",
    "          G_jobs_accom_food_added = G['jobs_accom_food_added'].sum()\n",
    "          G_jobs_manuf_added = G['jobs_manuf_added'].sum()\n",
    "          G_jobs_office_added = G['jobs_office_added'].sum()\n",
    "          G_jobs_other_added = G['jobs_other_added'].sum()\n",
    "          G_jobs_retail_added = G['jobs_retail_added'].sum()\n",
    "          G_jobs_wholesale_added = G['jobs_wholesale_added'].sum()\n",
    "\n",
    "          G_new_retail_jobs = G_jobs_accom_food_added + G_jobs_retail_added\n",
    "          G_new_industrial_jobs = G_jobs_manuf_added + G_jobs_wholesale_added\n",
    "          G_new_office_jobs = G_jobs_other_added + G_jobs_office_added + G_jobs_health_added + G_gov_edu_added\n",
    "          G_new_jobs = G_new_retail_jobs + G_new_industrial_jobs + G_new_office_jobs\n",
    "\n",
    "          # new jobs (non-Redev)\n",
    "          R_gov_edu_added = R['jobs_gov_edu_added'].sum()\n",
    "          R_jobs_health_added = R['jobs_health_added'].sum()\n",
    "          R_jobs_accom_food_added = R['jobs_accom_food_added'].sum()\n",
    "          R_jobs_manuf_added = R['jobs_manuf_added'].sum()\n",
    "          R_jobs_office_added = R['jobs_office_added'].sum()\n",
    "          R_jobs_other_added = R['jobs_other_added'].sum()\n",
    "          R_jobs_retail_added = R['jobs_retail_added'].sum()\n",
    "          R_jobs_wholesale_added = R['jobs_wholesale_added'].sum()\n",
    "\n",
    "          R_new_retail_jobs = R_jobs_accom_food_added + R_jobs_retail_added\n",
    "          R_new_industrial_jobs = R_jobs_manuf_added + R_jobs_wholesale_added\n",
    "          R_new_office_jobs = R_jobs_other_added + R_jobs_office_added + R_jobs_health_added + R_gov_edu_added\n",
    "          R_new_jobs = R_new_retail_jobs + R_new_industrial_jobs + R_new_office_jobs\n",
    "\n",
    "          # filter to new residential\n",
    "          G_new_sf = G[((G['was_developed']==1) | (G['was_redeveloped']==1)) & ((G['is_sf']==1))]\n",
    "          G_new_mf = G[((G['was_developed']==1) | (G['was_redeveloped']==1)) & ((G['is_mf']==1))]\n",
    "          R_new_sf = R[((R['was_developed']==1) | (R['was_redeveloped']==1)) & ((R['is_sf']==1))]\n",
    "          R_new_mf = R[((R['was_developed']==1) | (R['was_redeveloped']==1)) & ((R['is_mf']==1))]\n",
    "\n",
    "          # get the totals & counts\n",
    "          G_new_sf_units = G_new_sf['residential_units'].sum()\n",
    "          G_new_mf_units = G_new_mf['residential_units'].sum()\n",
    "          R_new_sf_units = R_new_sf['residential_units'].sum()\n",
    "          R_new_mf_units = R_new_mf['residential_units'].sum()\n",
    "          G_new_units = G_new_sf_units + G_new_mf_units\n",
    "          R_new_units = R_new_sf_units + R_new_mf_units\n",
    "\n",
    "\n",
    "          # append current data to final dataframe \n",
    "          d = {\n",
    "               'year': [year],\n",
    "\n",
    "               'C_new_res_units': [C_new_units], 'NC_new_res_units': [NC_new_units], \n",
    "               'C_new_sf_units': [C_new_sf_units], 'NC_new_sf_units': [NC_new_sf_units],\n",
    "               'C_new_mf_units': [C_new_mf_units], 'NC_new_mf_units': [NC_new_mf_units],\n",
    "               'C_new_jobs':[C_new_jobs], 'NC_new_jobs':[NC_new_jobs],\n",
    "               'C_new_retail_jobs':[C_new_retail_jobs], 'NC_new_retail_jobs':[NC_new_retail_jobs], \n",
    "               'C_new_industrial_jobs':[C_new_industrial_jobs],'NC_new_industrial_jobs':[NC_new_industrial_jobs], \n",
    "               'C_new_office_jobs':[C_new_office_jobs], 'NC_new_office_jobs':[NC_new_office_jobs],\n",
    "\n",
    "               'G_new_res_units': [G_new_units], 'R_new_res_units': [R_new_units], \n",
    "               'G_new_sf_units': [G_new_sf_units], 'R_new_sf_units': [R_new_sf_units],\n",
    "               'G_new_mf_units': [G_new_mf_units], 'R_new_mf_units': [R_new_mf_units],\n",
    "               'G_new_jobs':[G_new_jobs], 'R_new_jobs':[R_new_jobs],\n",
    "               'G_new_retail_jobs':[G_new_retail_jobs], 'R_new_retail_jobs':[R_new_retail_jobs], \n",
    "               'G_new_industrial_jobs':[G_new_industrial_jobs],'R_new_industrial_jobs':[R_new_industrial_jobs], \n",
    "               'G_new_office_jobs':[G_new_office_jobs], 'R_new_office_jobs':[R_new_office_jobs]\n",
    "               }\n",
    "\n",
    "          row = pd.DataFrame(data=d)\n",
    "          summary = pd.concat([summary, row]).astype(int)\n",
    "\n",
    "          # export\n",
    "          summary = summary[summary['year'] != 2019]\n",
    "          center_type_no_space = center_type.replace(' ', '_')\n",
    "          summary.to_csv(os.path.join(center_type_outputs, f'{center_type_no_space}_{name}.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11 [MSC v.1931 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3245673af07dcc28bdd829afb187282e9288a1f8195a5928b70ecba6e5973721"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
