{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  To do:\n",
    "- hh; jobs (office, industrial, retail); housing units (sf, mf)\n",
    "    - 2019 (average of runs)\n",
    "    - 2050 (average of runs)\n",
    "\n",
    "- append to parcel eq table\n",
    "    - create some pivots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import glob\n",
    "import numpy as np\n",
    "\n",
    "# show all columns\n",
    "pd.options.display.max_columns = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = os.path.join(r'.\\Outputs')\n",
    "if not os.path.exists(outputs):\n",
    "    os.makedirs(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parcel Equivalency Table\n",
    "eq = pd.read_csv(r\".\\Inputs\\parcel_eq_v5.csv\")\n",
    "\n",
    "# get 2019 and 2050 files\n",
    "remm_pm_2019_1 = pd.read_csv(r\"D:\\Josh_Projects\\REMM2_For_Python3_Internal_Use_1\\REMMRun\\Progression_Metrics\\run_48_year_2019_parcel_progression_metrics.csv\")\n",
    "remm_pm_2050_1 = pd.read_csv(r\"D:\\Josh_Projects\\REMM2_For_Python3_Internal_Use_1\\REMMRun\\Progression_Metrics\\run_48_year_2050_parcel_progression_metrics.csv\")\n",
    "\n",
    "remm_pm_2019_2 = pd.read_csv(r\"D:\\Josh_Projects\\REMM2_For_Python3_Internal_Use_2\\REMMRun\\Progression_Metrics\\run_48_year_2019_parcel_progression_metrics.csv\")\n",
    "remm_pm_2050_2 = pd.read_csv(r\"D:\\Josh_Projects\\REMM2_For_Python3_Internal_Use_2\\REMMRun\\Progression_Metrics\\run_48_year_2050_parcel_progression_metrics.csv\")\n",
    "\n",
    "remm_pm_2019_3 = pd.read_csv(r\"D:\\Josh_Projects\\REMM2_For_Python3_Internal_Use_3\\REMMRun\\Progression_Metrics\\run_48_year_2019_parcel_progression_metrics.csv\")\n",
    "remm_pm_2050_3 = pd.read_csv(r\"D:\\Josh_Projects\\REMM2_For_Python3_Internal_Use_3\\REMMRun\\Progression_Metrics\\run_48_year_2050_parcel_progression_metrics.csv\")\n",
    "\n",
    "remm_pm_2019_4 = pd.read_csv(r\"D:\\Josh_Projects\\REMM2_For_Python3_Internal_Use_4\\REMMRun\\Progression_Metrics\\run_47_year_2019_parcel_progression_metrics.csv\")\n",
    "remm_pm_2050_4 = pd.read_csv(r\"D:\\Josh_Projects\\REMM2_For_Python3_Internal_Use_4\\REMMRun\\Progression_Metrics\\run_47_year_2050_parcel_progression_metrics.csv\")\n",
    "\n",
    "remm_pm_2019_5 = pd.read_csv(r\"D:\\Josh_Projects\\REMM2_For_Python3_Internal_Use_5\\REMMRun\\Progression_Metrics\\run_47_year_2019_parcel_progression_metrics.csv\")\n",
    "remm_pm_2050_5 = pd.read_csv(r\"D:\\Josh_Projects\\REMM2_For_Python3_Internal_Use_5\\REMMRun\\Progression_Metrics\\run_47_year_2050_parcel_progression_metrics.csv\")\n",
    "\n",
    "remm_pm_2019_6 = pd.read_csv(r\"D:\\Josh_Projects\\REMM2_For_Python3_Internal_Use_6\\REMMRun\\Progression_Metrics\\run_46_year_2019_parcel_progression_metrics.csv\")\n",
    "remm_pm_2050_6 = pd.read_csv(r\"D:\\Josh_Projects\\REMM2_For_Python3_Internal_Use_6\\REMMRun\\Progression_Metrics\\run_46_year_2050_parcel_progression_metrics.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_df(df, year):\n",
    "    df = df.set_index('parcel_id')\n",
    "    df.loc[(df['is_sf']==1), 'sf_units'] = df['residential_units']\n",
    "    df.loc[(df['is_mf']==1), 'mf_units'] = df['residential_units']\n",
    "    df['ind_jobs'] = df['jobs_wholesale'] + df['jobs_manuf']\n",
    "    df['rtl_jobs'] = df['jobs_retail'] + df['jobs_accom_food']\n",
    "    df['off_jobs'] = df['jobs_office'] + df['jobs_gov_edu'] + df['jobs_health'] + df['jobs_other']\n",
    "    df.loc[(df['has_buildings'] != 1), 'vacant_acres'] = df['parcel_acres']\n",
    "    df.loc[(df['has_buildings'] != 1) & (df['developable'] == 1), 'vacant_devacres'] = df['parcel_acres']\n",
    "    df['vacant_acres'].fillna(0, inplace=True)\n",
    "    df['vacant_devacres'].fillna(0, inplace=True)\n",
    "\n",
    "    df = df[['sf_units', 'mf_units', 'job_spaces', 'ind_jobs', 'rtl_jobs', 'off_jobs', 'vacant_acres', 'vacant_devacres']].copy()\n",
    "    new_col_names = [col + f'_{str(year)[2:]}' for col in list(df.columns)]\n",
    "    df.columns = new_col_names\n",
    "    return df.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "remm_pm_2019 = [remm_pm_2019_1, remm_pm_2019_2, remm_pm_2019_3, remm_pm_2019_4, remm_pm_2019_5, remm_pm_2019_6]\n",
    "remm_pm_2050 = [remm_pm_2050_1, remm_pm_2050_2, remm_pm_2050_3, remm_pm_2050_4, remm_pm_2050_5, remm_pm_2050_6]\n",
    "\n",
    "remm_pm_2019_processed = [prepare_df(df, 2019) for df in remm_pm_2019]\n",
    "remm_pm_2050_processed = [prepare_df(df, 2050) for df in remm_pm_2050]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stack all 6 runs together\n",
    "data_stack_2019 = pd.concat(remm_pm_2019_processed)\n",
    "data_stack_2050 = pd.concat(remm_pm_2050_processed)\n",
    "\n",
    "# average the 6 runs\n",
    "results_2019 = data_stack_2019.groupby(data_stack_2019.index).mean().reset_index().round().astype(int)\n",
    "results_2050 = data_stack_2050.groupby(data_stack_2050.index).mean().reset_index().round().astype(int)\n",
    "\n",
    "results_2019['residential_units_19'] = results_2019['sf_units_19'] + results_2019['mf_units_19']\n",
    "results_2019['ttl_jobs_19'] = results_2019['off_jobs_19'] + results_2019['rtl_jobs_19'] + results_2019['ind_jobs_19']\n",
    "results_2050['residential_units_50'] = results_2050['sf_units_50'] + results_2050['mf_units_50']\n",
    "results_2050['ttl_jobs_50'] = results_2050['off_jobs_50'] + results_2050['rtl_jobs_50'] + results_2050['ind_jobs_50']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill na's in parcel eq\n",
    "eq.fillna({'CENTER_NAME':'Non-Center', 'CENTER_TYPE':'Non-Center', 'CITY_AREA':'Non-City-Area'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "results = eq.merge(results_2019, on='parcel_id', how='left').merge(results_2050, on='parcel_id', how='left')\n",
    "results.to_csv(os.path.join(outputs, 'SE_by_PARCEL.csv'), index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eq.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['parcel_id', 'TAZID_832', 'TAZID_900', 'CENTER_NAME', 'CENTER_TYPE',\n",
       "       'CITY_AREA', 'COUNTY_NAME', 'COUNTY_FIPS', 'MPO', 'INFILL1990',\n",
       "       'parcel_acres', 'sf_units_19', 'mf_units_19', 'job_spaces_19',\n",
       "       'ind_jobs_19', 'rtl_jobs_19', 'off_jobs_19', 'vacant_acres_19',\n",
       "       'vacant_devacres_19', 'residential_units_19', 'ttl_jobs_19',\n",
       "       'sf_units_50', 'mf_units_50', 'job_spaces_50', 'ind_jobs_50',\n",
       "       'rtl_jobs_50', 'off_jobs_50', 'vacant_acres_50', 'vacant_devacres_50',\n",
       "       'residential_units_50', 'ttl_jobs_50'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aggregate to different geographies\n",
    "data_columns = ['parcel_acres', 'residential_units_19', 'sf_units_19', 'mf_units_19', \n",
    "                'job_spaces_19', 'ind_jobs_19', 'rtl_jobs_19', 'off_jobs_19', 'ttl_jobs_19', 'vacant_acres_19', 'vacant_devacres_19',\n",
    "                'residential_units_50', 'sf_units_50', 'mf_units_50', 'job_spaces_50',\n",
    "                'ind_jobs_50', 'rtl_jobs_50', 'off_jobs_50','ttl_jobs_50', 'vacant_acres_50','vacant_devacres_50']\n",
    "\n",
    "# aggregations\n",
    "results.groupby('CITY_AREA')[data_columns].sum().reset_index().to_csv(os.path.join(outputs, 'SE_by_CITY_AREA.csv'), index = False)\n",
    "results.groupby('COUNTY_NAME')[data_columns].sum().reset_index().to_csv(os.path.join(outputs, 'SE_by_COUNTY.csv'), index = False)\n",
    "results.groupby('CENTER_NAME')[data_columns].sum().reset_index().to_csv(os.path.join(outputs, 'SE_by_CENTER_NAME.csv'), index = False)\n",
    "results.groupby('CENTER_TYPE')[data_columns].sum().reset_index().to_csv(os.path.join(outputs, 'SE_by_CENTER_TYPE.csv'), index = False)\n",
    "results.groupby('INFILL1990')[data_columns].sum().reset_index().to_csv(os.path.join(outputs, 'SE_by_INFILL1990.csv'), index = False)\n",
    "results.groupby('MPO')[data_columns].sum().reset_index().to_csv(os.path.join(outputs, 'SE_by_MPO.csv'), index = False)\n",
    "\n",
    "# pivots\n",
    "results.pivot_table(index='CENTER_TYPE', columns='COUNTY_NAME', values=data_columns,  aggfunc='sum').fillna(0).reset_index().to_csv(os.path.join(outputs, 'SE_by_CENTER_TYPE_by_COUNTY.csv'), index = False)\n",
    "results.pivot_table(index='COUNTY_NAME', columns='INFILL1990', values=data_columns,  aggfunc='sum').fillna(0).reset_index().to_csv(os.path.join(outputs, 'SE_by_INFILL1990_by_COUNTY.csv'), index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# taz 9.0 summary for MB\n",
    "results['res_units_per_acre_19'] = round(results['residential_units_19'] / results['parcel_acres'], 2)\n",
    "results['sf_units_per_acre_19'] = round(results['sf_units_19'] / results['parcel_acres'], 2)\n",
    "results['mf_units_per_acre_19'] = round(results['mf_units_19'] / results['parcel_acres'], 2)\n",
    "results['job_spaces_per_acre_19'] = round(results['job_spaces_19'] / results['parcel_acres'], 2)\n",
    "results['ttl_jobs_per_acre_19'] = round(results['ttl_jobs_19'] / results['parcel_acres'], 2)\n",
    "results['ind_jobs_per_acre_19'] = round(results['ind_jobs_19'] / results['parcel_acres'], 2)\n",
    "results['rtl_jobs_per_acre_19'] = round(results['rtl_jobs_19'] / results['parcel_acres'], 2)\n",
    "results['off_jobs_per_acre_19'] = round(results['off_jobs_19'] / results['parcel_acres'], 2)\n",
    "\n",
    "results['res_units_per_acre_50'] = round(results['residential_units_50'] / results['parcel_acres'], 2)\n",
    "results['sf_units_per_acre_50'] = round(results['sf_units_50'] / results['parcel_acres'], 2)\n",
    "results['mf_units_per_acre_50'] = round(results['mf_units_50'] / results['parcel_acres'], 2)\n",
    "results['job_spaces_per_acre_50'] = round(results['job_spaces_50'] / results['parcel_acres'], 2)\n",
    "results['ttl_jobs_per_acre_50'] = round(results['ttl_jobs_50'] / results['parcel_acres'], 2)\n",
    "results['ind_jobs_per_acre_50'] = round(results['ind_jobs_50'] / results['parcel_acres'], 2)\n",
    "results['rtl_jobs_per_acre_50'] = round(results['rtl_jobs_50'] / results['parcel_acres'], 2)\n",
    "results['off_jobs_per_acre_50'] = round(results['off_jobs_50'] / results['parcel_acres'], 2)\n",
    "\n",
    "density_columns = ['res_units_per_acre_19', 'res_units_per_acre_50',\n",
    "                   'sf_units_per_acre_19', 'sf_units_per_acre_50',\n",
    "                   'mf_units_per_acre_19', 'mf_units_per_acre_50',\n",
    "                   'job_spaces_per_acre_19', 'job_spaces_per_acre_50', \n",
    "                   'ttl_jobs_per_acre_19', 'ttl_jobs_per_acre_50', \n",
    "                   'ind_jobs_per_acre_19', 'ind_jobs_per_acre_50', \n",
    "                   'rtl_jobs_per_acre_19', 'rtl_jobs_per_acre_50', \n",
    "                   'off_jobs_per_acre_19', 'off_jobs_per_acre_50']\n",
    "\n",
    "# density using aggregated values\n",
    "taz_sum = results.groupby('TAZID_900')[data_columns].sum().reset_index()\n",
    "taz_sum['res_units_per_acre_19'] = round(taz_sum['residential_units_19'] / taz_sum['parcel_acres'], 2)\n",
    "taz_sum['sf_units_per_acre_19'] = round(taz_sum['sf_units_19'] / taz_sum['parcel_acres'], 2)\n",
    "taz_sum['mf_units_per_acre_19'] = round(taz_sum['mf_units_19'] / taz_sum['parcel_acres'], 2)\n",
    "taz_sum['job_spaces_per_acre_19'] = round(taz_sum['job_spaces_19'] / taz_sum['parcel_acres'], 2)\n",
    "taz_sum['ttl_jobs_per_acre_19'] = round(taz_sum['ttl_jobs_19'] / taz_sum['parcel_acres'], 2)\n",
    "taz_sum['ind_jobs_per_acre_19'] = round(taz_sum['ind_jobs_19'] / taz_sum['parcel_acres'], 2)\n",
    "taz_sum['rtl_jobs_per_acre_19'] = round(taz_sum['rtl_jobs_19'] / taz_sum['parcel_acres'], 2)\n",
    "taz_sum['off_jobs_per_acre_19'] = round(taz_sum['off_jobs_19'] / taz_sum['parcel_acres'], 2)\n",
    "\n",
    "taz_sum['res_units_per_acre_50'] = round(taz_sum['residential_units_50'] / taz_sum['parcel_acres'], 2)\n",
    "taz_sum['sf_units_per_acre_50'] = round(taz_sum['sf_units_50'] / taz_sum['parcel_acres'], 2)\n",
    "taz_sum['mf_units_per_acre_50'] = round(taz_sum['mf_units_50'] / taz_sum['parcel_acres'], 2)\n",
    "taz_sum['job_spaces_per_acre_50'] = round(taz_sum['job_spaces_50'] / taz_sum['parcel_acres'], 2)\n",
    "taz_sum['ttl_jobs_per_acre_50'] = round(taz_sum['ttl_jobs_50'] / taz_sum['parcel_acres'], 2)\n",
    "taz_sum['ind_jobs_per_acre_50'] = round(taz_sum['ind_jobs_50'] / taz_sum['parcel_acres'], 2)\n",
    "taz_sum['rtl_jobs_per_acre_50'] = round(taz_sum['rtl_jobs_50'] / taz_sum['parcel_acres'], 2)\n",
    "taz_sum['off_jobs_per_acre_50'] = round(taz_sum['off_jobs_50'] / taz_sum['parcel_acres'], 2)\n",
    "\n",
    "taz_sum = taz_sum[['TAZID_900','parcel_acres', 'vacant_acres_19', 'vacant_acres_50', 'vacant_devacres_19', 'vacant_devacres_50'] + density_columns].copy()\n",
    "taz_sum.set_index('TAZID_900', inplace=True)\n",
    "\n",
    "# max and mean densities of parcels within taz\n",
    "taz_max = results.groupby('TAZID_900')[density_columns].max()\n",
    "max_col_names = ['max_' + col for col in list(taz_max.columns)]\n",
    "taz_max.columns = max_col_names\n",
    "\n",
    "taz_mean = round(results.groupby('TAZID_900')[density_columns].mean(), 2)\n",
    "mean_col_names = ['mean_' + col for col in list(taz_mean.columns)]\n",
    "taz_mean.columns = mean_col_names\n",
    "\n",
    "# export\n",
    "taz = taz_sum.merge(taz_max, left_index=True, right_index=True, how='left').merge(taz_mean, left_index=True, right_index=True, how='left')\n",
    "taz.replace([np.inf, -np.inf, np.nan], 0, inplace=True)\n",
    "taz.to_csv(os.path.join(outputs, 'SE_Density_TAZ900.csv'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11 [MSC v.1931 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3245673af07dcc28bdd829afb187282e9288a1f8195a5928b70ecba6e5973721"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
